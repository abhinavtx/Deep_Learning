{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense,LSTM,GRU, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are loading the imdb data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abhinav.aggarwal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Now we have to make a deep rnn\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    ##  Input is of 10000 and in output every word is displayed by vector of length 32 (output_dim)\n",
    "    SimpleRNN(5, return_sequences= True),\n",
    "    ## In this we are adding a RNN layer with 5 units and also we have made return_sequences = True as we have want to make deep rnn\n",
    "    ## As we want to give it to next layer\n",
    "    SimpleRNN(5),                          \n",
    "    Dense(1, activation='sigmoid') # Now we have added one dense layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100, 5)            190       \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 5)                 55        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320251 (1.22 MB)\n",
      "Trainable params: 320251 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abhinav.aggarwal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\abhinav.aggarwal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\abhinav.aggarwal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 28s 40ms/step - loss: 0.5268 - accuracy: 0.7351 - val_loss: 0.4785 - val_accuracy: 0.7774\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3696 - accuracy: 0.8468 - val_loss: 0.4325 - val_accuracy: 0.8118\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.2927 - accuracy: 0.8861 - val_loss: 0.4539 - val_accuracy: 0.8074\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2337 - accuracy: 0.9146 - val_loss: 0.4634 - val_accuracy: 0.8140\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.1793 - accuracy: 0.9360 - val_loss: 0.5057 - val_accuracy: 0.8034\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we shall be making for stacked rnn's\n",
    "## The same above concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 5)            760       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 5)                 220       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320986 (1.22 MB)\n",
      "Trainable params: 320986 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model2 = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    LSTM(5, return_sequences=True),\n",
    "    LSTM(5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 28s 39ms/step - loss: 0.4749 - accuracy: 0.7794 - val_loss: 0.4053 - val_accuracy: 0.8278\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 0.2910 - accuracy: 0.8892 - val_loss: 0.3715 - val_accuracy: 0.8404\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.2044 - accuracy: 0.9262 - val_loss: 0.4421 - val_accuracy: 0.8234\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 0.1648 - accuracy: 0.9444 - val_loss: 0.4426 - val_accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.1242 - accuracy: 0.9600 - val_loss: 0.4779 - val_accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history2 = model2.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 5)            585       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 5)                 180       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320771 (1.22 MB)\n",
      "Trainable params: 320771 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the GRU model\n",
    "model3 = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    GRU(5, return_sequences=True),\n",
    "    GRU(5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 33s 48ms/step - loss: 0.5028 - accuracy: 0.7458 - val_loss: 0.4249 - val_accuracy: 0.8206\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 42s 66ms/step - loss: 0.3076 - accuracy: 0.8793 - val_loss: 0.3797 - val_accuracy: 0.8370\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 39s 62ms/step - loss: 0.2344 - accuracy: 0.9132 - val_loss: 0.3930 - val_accuracy: 0.8340\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1925 - accuracy: 0.9314 - val_loss: 0.4142 - val_accuracy: 0.8302\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.1582 - accuracy: 0.9467 - val_loss: 0.4883 - val_accuracy: 0.8266\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history3 = model3.fit(x_train, y_train, epochs = 5, validation_split = 0.2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now for creating a bidirectional rnn, we just have to add a biderectional wrapper around the rnn layer command\n",
    "model4 = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    ##  Input is of 10000 and in output every word is displayed by vector of length 32 (output_dim)\n",
    "    Bidirectional(SimpleRNN(5)),\n",
    "    ## In this we are adding a RNN layer with 5 units and also we have made return_sequences = True as we have want to make deep rnn             \n",
    "    Dense(1, activation='sigmoid') # Now we have added one dense layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 10)                380       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320391 (1.22 MB)\n",
      "Trainable params: 320391 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 22s 32ms/step - loss: 0.6102 - accuracy: 0.6612 - val_loss: 0.5091 - val_accuracy: 0.7626\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.3932 - accuracy: 0.8291 - val_loss: 0.4354 - val_accuracy: 0.8056\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 25s 39ms/step - loss: 0.2507 - accuracy: 0.9042 - val_loss: 0.4653 - val_accuracy: 0.8008\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.1399 - accuracy: 0.9554 - val_loss: 0.5029 - val_accuracy: 0.7976\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.0853 - accuracy: 0.9756 - val_loss: 0.5580 - val_accuracy: 0.7852\n"
     ]
    }
   ],
   "source": [
    "model4.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history4 = model4.fit(x_train, y_train, epochs = 5, validation_split = 0.2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The concept of bidirectional is used in both lstm and gru.\n",
    "\n",
    "## Bidirectional LSTM are used more than bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional have more overfitting\n",
    "## So to reduce overfitting we can apply Dropout, Regularization and other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
